{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegNet.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4HjlOCwaZzPq",
        "colab_type": "code",
        "outputId": "539b2ccb-c960-4e60-f19d-9a8f1fd4fd4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_pbGVddPQduR",
        "colab_type": "code",
        "outputId": "3b412000-e59c-4751-c261-6620da27121b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from numpy import array\n",
        "import keras\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G8ndO35MQi4X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "x1 = tf.placeholder(tf.float32)\n",
        "y1 = tf.placeholder(tf.float32)\n",
        "z1 = tf.placeholder(tf.float32)\n",
        "x2 = tf.placeholder(tf.float32)\n",
        "y2 = tf.placeholder(tf.float32)\n",
        "z2 = tf.placeholder(tf.float32)\n",
        "cube = tf.placeholder(tf.float32)\n",
        "lr = tf.placeholder(tf.float32)\n",
        "Y_ = tf.placeholder(tf.int32)\n",
        "\n",
        "x1 = tf.reshape(x1, shape=[1,29,29,1])\n",
        "y1 = tf.reshape(y1, shape=[1,29,29,1])\n",
        "z1 = tf.reshape(z1, shape=[1,29,29,1])\n",
        "x2 = tf.reshape(x2, shape=[1,87,87,1])\n",
        "y2 = tf.reshape(y2, shape=[1,87,87,1])\n",
        "z2 = tf.reshape(z2, shape=[1,87,87,1])\n",
        "cube = tf.reshape(cube, shape=[1,13,13,13,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BuaKJeUe8yA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv3d(x, n_kernels):\n",
        "  return tf.layers.conv3d(x,n_kernels,(5,5,5),padding=\"valid\",activation=tf.nn.elu)\n",
        "\n",
        "def conv2d(x, n_kernels):\n",
        "  return tf.layers.conv2d(x,n_kernels,(5,5),padding=\"valid\",activation=tf.nn.elu)\n",
        "\n",
        "def conv2d_shared(x, n_kernels,nam):\n",
        "  return tf.layers.conv2d(x,n_kernels,(5,5),padding=\"valid\",name=nam)\n",
        "\n",
        "def maxpool(x):\n",
        "  return tf.layers.max_pooling2d(x,(2,2),padding=\"valid\")\n",
        "\n",
        "def meanpool(x):\n",
        "  return tf.layers.average_pooling2d(x,(3,3),strides=3)\n",
        "\n",
        "def batchnorm(x):\n",
        "  return tf.contrib.layers.batch_norm(x, decay= 0.99, is_training=True,center= True, scale=True, reuse= False)\n",
        "    \n",
        "def relu(x):\n",
        "  return tf.nn.relu(x)\n",
        "\n",
        "def residual(x,y):\n",
        "  n_x = x.shape[-1]\n",
        "  n_y = y.shape[-1]\n",
        "  spatial_rank = 3\n",
        "\n",
        "  if n_x > n_y:  # pad the channel dim\n",
        "      pad_1 = np.int((n_x - n_y) // 2)\n",
        "      pad_2 = np.int(n_x - n_y - pad_1)\n",
        "      padding_dims = np.vstack(([[0, 0]], [[0, 0]] * spatial_rank, [[pad_1, pad_2]]))\n",
        "      y = tf.pad(tensor=y, paddings=padding_dims.tolist(), mode='CONSTANT')\n",
        "  elif n_x < n_y:  # make a projection\n",
        "      y = conv3d(y,1)\n",
        "\n",
        "  # element-wise sum of both paths\n",
        "  output_tensor = x + y\n",
        "  \n",
        "\n",
        "  return output_tensor\n",
        "\n",
        "def dilatedCon(x,n_kernels,dilation):\n",
        "  return tf.layers.conv3d(x, n_kernels,(3,3,3), [1, 1, 1], padding=\"same\", dilation_rate=[dilation, dilation, dilation])\n",
        "\n",
        "def finalconv(x):\n",
        "  return tf.layers.conv3d(x,1,(1,1,1),padding=\"same\")\n",
        "\n",
        "def softmax(x):\n",
        "  return tf.nn.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1QexR4f4GoSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  with tf.variable_scope(\"conv1\",reuse=tf.AUTO_REUSE):\n",
        "    x1_1 = conv2d_shared(x1,20,\"layer1\")\n",
        "    y1_1 = conv2d_shared(y1,20,\"layer1\")\n",
        "    z1_1 = conv2d_shared(z1,20,\"layer1\")\n",
        "  with tf.variable_scope(\"conv2\",reuse=tf.AUTO_REUSE):\n",
        "    x2_1 = conv2d_shared(meanpool(x2),20,\"layer2\")\n",
        "    y2_1 = conv2d_shared(meanpool(y2),20,\"layer2\")\n",
        "    z2_1 = conv2d_shared(meanpool(z2),20,\"layer2\")\n",
        "\n",
        "  cube_1 = conv3d(cube,20)\n",
        "\n",
        "  x1_2 = conv2d(x1_1,50)\n",
        "  y1_2 = conv2d(y1_1,50)\n",
        "  z1_2 = conv2d(z1_1,50)\n",
        "  x2_2 = conv2d(x2_1,50)\n",
        "  y2_2 = conv2d(y2_1,50)\n",
        "  z2_2 = conv2d(z2_1,50)\n",
        "  cube_2 = conv3d(cube_1,50)\n",
        "\n",
        "  x1_2 = tf.layers.flatten(x1_2)\n",
        "  y1_2 = tf.layers.flatten(y1_2)\n",
        "  z1_2 = tf.layers.flatten(z1_2)\n",
        "  x2_2 = tf.layers.flatten(x2_2)\n",
        "  y2_2 = tf.layers.flatten(y2_2)\n",
        "  z2_2 = tf.layers.flatten(z2_2)\n",
        "  cube_2 = tf.layers.flatten(cube_2)\n",
        "\n",
        "  conv = tf.concat([x1_2, y1_2, z1_2, x2_2, y2_2, z2_2, cube_2],axis=1)\n",
        "  fc1 = tf.contrib.layers.fully_connected(conv,3000,activation_fn=tf.nn.elu)\n",
        "  fc2 = tf.contrib.layers.fully_connected(fc1, 3000,activation_fn=tf.nn.elu)\n",
        "  logits = tf.contrib.layers.fully_connected(fc2,135,activation_fn=tf.nn.elu)\n",
        "  soft_logits = tf.nn.softmax(logits)\n",
        "\n",
        "  #fc2 = tf.nn.dropout(fc2, keep_prob)\n",
        "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tf.cast(Y_,tf.int32)))\n",
        "  optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
        "  #optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NeVXf2eTag8p",
        "colab_type": "code",
        "outputId": "e8c77099-5481-480b-d9bf-a340294548cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()\n",
        "gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "sess.run(init)\n",
        "\n",
        "def whitening(image):\n",
        "    \"\"\"Whitening. Normalises image to zero mean and unit variance.\"\"\"\n",
        "\n",
        "    image = image.astype(np.float32)\n",
        "\n",
        "    mean = np.mean(image)\n",
        "    std = np.std(image)\n",
        "\n",
        "    if std > 0:\n",
        "        ret = (image - mean) / std\n",
        "    else:\n",
        "        ret = image * 0.\n",
        "    return ret\n",
        "\n",
        "\n",
        "dataset = np.load(\"drive/My Drive/Colab Notebooks/Dataset_final_1.npy\")\n",
        "npad = ((43,43), (43,43), (43,43))\n",
        "#for i in range (0, len(dataset)):\n",
        "#  dataset[i][0] = whitening(dataset[i][0])\n",
        "print(\"dataset length \",len(dataset))\n",
        "training_set = []\n",
        "\n",
        "hm_epochs = 1\n",
        "starting = 0\n",
        "\n",
        "if(starting>0):\n",
        "  saver.restore(sess, \"drive/My Drive/Colab Notebooks/Training Models/model-\"+ str(starting)+\".ckpt\")\n",
        "  print(\"Model Restored\")\n",
        "print(\"Start Training\")\n",
        "try:\n",
        "  for epoch in range(hm_epochs):\n",
        "      #print(\"Start Training\")\n",
        "      correct = 0\n",
        "      iter = 0\n",
        "      stime = time.time()\n",
        "      for i in range(0,len(dataset)):\n",
        "        total = np.size(dataset[i][0])\n",
        "        print(\"File \" + str(i))\n",
        "        for j in range (0,1000):\n",
        "          value = 0\n",
        "          stime = time.time()\n",
        "          \"\"\"\n",
        "          voxel = random.randint(0,total)\n",
        "\n",
        "          #find the voxel x,y,z values\n",
        "          z_layer = (voxel//65536)\n",
        "          row = (voxel%65536)//256\n",
        "          element = (voxel%65536)%256\n",
        "          value = dataset[i][1][z_layer][row][element]\n",
        "          \"\"\"\n",
        "          while(value==0):\n",
        "            voxel = random.randint(0,total)\n",
        "            output = np.zeros((1,135),dtype=np.int32)\n",
        "\n",
        "            #find the voxel x,y,z values\n",
        "            z_layer = (voxel//65536)\n",
        "            row = (voxel%65536)//256\n",
        "            element = (voxel%65536)%256\n",
        "            value = dataset[i][1][z_layer][row][element]\n",
        "\n",
        "          \n",
        "          raw = np.pad(dataset[i][0], pad_width=npad, mode='constant', constant_values=0)\n",
        "          #raw = raw/5500\n",
        "          raw = whitening(raw)\n",
        "          n_values = 135\n",
        "          output = [value]\n",
        "\n",
        "          small_x = raw[z_layer+43][row+29:row+58,element+29:element+58]\n",
        "          small_y = raw[z_layer+29:z_layer+58,row+29:row+58,element+43]\n",
        "          small_z = raw[z_layer+29:z_layer+58,row+43,element+29:element+58]\n",
        "          large_x = raw[z_layer][row:row+87,element:element+87]\n",
        "          large_y = raw[z_layer:z_layer+87,row:row+87,element]\n",
        "          large_z = raw[z_layer:z_layer+87,row,element:element+87]\n",
        "          vol = raw[z_layer+37:z_layer+50,row+37:row+50,element+37:element+50]\n",
        "\n",
        "\n",
        "          small_x = np.reshape(small_x, (1,29,29,1))\n",
        "          small_y = np.reshape(small_y, (1,29,29,1))\n",
        "          small_z = np.reshape(small_z, (1,29,29,1))\n",
        "          large_x = np.reshape(large_x, (1,87,87,1))\n",
        "          large_y = np.reshape(large_y, (1,87,87,1))\n",
        "          large_z = np.reshape(large_z, (1,87,87,1))\n",
        "          vol = np.reshape(vol, (1,13,13,13,1))\n",
        "          pred = (soft_logits.eval({x1: small_x, y1: small_y, z1: small_z, x2: large_x, y2: large_y, z2: large_z, cube: vol},session=sess))\n",
        "          #print(pred)\n",
        "          print(\"Precited: \", np.argmax(pred))\n",
        "          _,loss_value = sess.run([optimizer, loss], feed_dict={x1: small_x, y1: small_y, z1: small_z, x2: large_x, y2: large_y, z2: large_z, cube: vol , Y_ : output, lr: 0.001})\n",
        "          print(\"Class: \", value)\n",
        "          #output the probabilities\n",
        "          #pred = (soft_logits.eval({x1: small_x, y1: small_y, z1: small_z, x2: large_x, y2: large_y, z2: large_z, cube: vol},session=sess))\n",
        "          #print(pred)\n",
        "          #print(\"Precited: \", np.argmax(pred))\n",
        "          if(value==np.argmax(pred)):\n",
        "            correct += 1\n",
        "          iter += 1\n",
        "          print(\"Correct Prediction % : \", (correct/iter)*100)\n",
        "          if((j+1)%100 == 0):\n",
        "            print(str(j+1) + \" complete out of 20000. Loss is: \" + str(loss_value))\n",
        "          if((j+1)%5000 == 0):\n",
        "            print(\"Training 5000 iterations complete Saving Model with \" + str(j+1) +\" Iterations\")\n",
        "            save_path = saver.save(sess, \"drive/My Drive/Colab Notebooks/Training Models/model-\"+ str(j+1)+\".ckpt\")\n",
        "            print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "            \n",
        "          #print(stime-time.time())\n",
        "            \n",
        "  print(\"One Epoch Completed: Time Taken %s s\" % (time.time()-stime))\n",
        "  stime = time.time()\n",
        "  print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',str(loss_value))\n",
        "\n",
        "\n",
        "  print(\"Training Complete Saving Model with \" + str(j+1) +\" Iterations\")\n",
        "  save_path = saver.save(sess, \"drive/My Drive/Colab Notebooks/Training Models/model-\"+ str(j+1)+\".ckpt\")\n",
        "  print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "\n",
        "  print(\"Done!\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"Training Interrupted Saving Model with \" + str(j+1+starting) +\" Iterations\")\n",
        "  #save_path = saver.save(sess, \"drive/My Drive/Colab Notebooks/Training Models/model-\"+ str(j+1+starting)+\".ckpt\")\n",
        "  #print(\"Model saved in path: %s\" % save_path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset length  5\n",
            "Start Training\n",
            "File 0\n",
            "Precited:  127\n",
            "Class:  98.0\n",
            "Correct Prediction % :  0.0\n",
            "Precited:  98\n",
            "Class:  15.0\n",
            "Correct Prediction % :  0.0\n",
            "Precited:  15\n",
            "Class:  15.0\n",
            "Correct Prediction % :  33.33333333333333\n",
            "Training Interrupted Saving Model with 4 Iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qOqOzshZuUwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the Evaluation Code"
      ]
    },
    {
      "metadata": {
        "id": "4CXo3X8KAIcx",
        "colab_type": "code",
        "outputId": "de5ca6d0-2b25-440f-d3c4-7ada2de17c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(1,len(dataset)):\n",
        "  total = np.size(dataset[i][0])\n",
        "  print(\"File \" + str(i))\n",
        "  iter = 0\n",
        "  correct = 0\n",
        "  output = np.zeros((65536,), dtype=int)\n",
        "  for j in range (6553600,6619137):\n",
        "    stime = time.time()\n",
        "    voxel = j\n",
        "    print(j)\n",
        "    \n",
        "    \n",
        "    \n",
        "    #find the voxel x,y,z values\n",
        "    z_layer = (voxel//65536)\n",
        "    row = (voxel%65536)//256\n",
        "    element = (voxel%65536)%256\n",
        "    raw = np.pad(dataset[i][0], pad_width=npad, mode='constant', constant_values=0)\n",
        "    raw = whitening(raw)\n",
        "    value = dataset[i][1][z_layer][row][element]\n",
        "    if(value==0):\n",
        "      iter += 1\n",
        "      output[iter] = 0\n",
        "      continue\n",
        "    \n",
        "    small_x = raw[z_layer+43][row+29:row+58,element+29:element+58]\n",
        "    small_y = raw[z_layer+29:z_layer+58,row+29:row+58,element+43]\n",
        "    small_z = raw[z_layer+29:z_layer+58,row+43,element+29:element+58]\n",
        "    large_x = raw[z_layer][row:row+87,element:element+87]\n",
        "    large_y = raw[z_layer:z_layer+87,row:row+87,element]\n",
        "    large_z = raw[z_layer:z_layer+87,row,element:element+87]\n",
        "    vol = raw[z_layer+37:z_layer+50,row+37:row+50,element+37:element+50]\n",
        "    \n",
        "    small_x = np.reshape(small_x, (1,29,29,1))\n",
        "    small_y = np.reshape(small_y, (1,29,29,1))\n",
        "    small_z = np.reshape(small_z, (1,29,29,1))\n",
        "    large_x = np.reshape(large_x, (1,87,87,1))\n",
        "    large_y = np.reshape(large_y, (1,87,87,1))\n",
        "    large_z = np.reshape(large_z, (1,87,87,1))\n",
        "    vol = np.reshape(vol, (1,13,13,13,1))\n",
        "    \n",
        "\n",
        "    pred = (soft_logits.eval({x1: small_x, y1: small_y, z1: small_z, x2: large_x, y2: large_y, z2: large_z, cube: vol},session=sess))\n",
        "    #print(pred)\n",
        "    print(str(iter+1) + \" out of \" + str(total))\n",
        "    print(\"Value: \", value)\n",
        "    print(\"Precited: \", np.argmax(pred))\n",
        "    \n",
        "    output[iter] = np.argmax(pred)\n",
        "    \n",
        "    if(value==np.argmax(pred)):\n",
        "      correct += 1\n",
        "    iter += 1\n",
        "    print(\"Correct Prediction % : \", (correct/iter)*100)\n",
        "    print(\"One Voxel Completed: Time Taken %s s\" % (time.time()-stime))\n",
        "    stime = time.time()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File 1\n",
            "6553600\n",
            "6553601\n",
            "6553602\n",
            "6553603\n",
            "6553604\n",
            "6553605\n",
            "6553606\n",
            "6553607\n",
            "6553608\n",
            "6553609\n",
            "6553610\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b4ba55b5c98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvoxel\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitening\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-090ee9ba31fc>\u001b[0m in \u001b[0;36mwhitening\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oJmGrqEigzdM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "true_labels = [4, 11, 23, 30, 31, 32, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 55, 56, 57,\n",
        "                   58, 59, 60, 61, 62, 69, 71, 72, 73, 75, 76, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112,\n",
        "                   113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 128, 129, 132, 133, 134, 135, 136,\n",
        "                   137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
        "                   157, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
        "                   179, 180, 181, 182, 183, 184, 185, 186, 187, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200,\n",
        "                   201, 202, 203, 204, 205, 206, 207]\n",
        "\n",
        "print(len(true_labels))\n",
        "\n",
        "onehot_labels=tf.one_hot(indices=[1],depth=134)\n",
        "print(onehot_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4H_0mQyeLfR_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval(true,predicted):\n",
        "  true_labels = [4, 11, 23, 30, 31, 32, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 55, 56, 57,\n",
        "                   58, 59, 60, 61, 62, 69, 71, 72, 73, 75, 76, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 112,\n",
        "                   113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 128, 129, 132, 133, 134, 135, 136,\n",
        "                   137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
        "                   157, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
        "                   179, 180, 181, 182, 183, 184, 185, 186, 187, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200,\n",
        "                   201, 202, 203, 204, 205, 206, 207]\n",
        "  \n",
        "  dices = []\n",
        "  tester = []\n",
        "  \n",
        "  \n",
        "  for num in (true_labels):\n",
        "    if (num in true):\n",
        "      masker = (true==num)\n",
        "      true_mask = masker*true\n",
        "      pred_mask = masker*predicted\n",
        "      numerator = 2*np.count_nonzero(pred_mask==num)\n",
        "      \n",
        "      denominator = np.count_nonzero(true==num)+np.count_nonzero(predicted==num)\n",
        "      dice = numerator/denominator\n",
        "      dices.append(dice)\n",
        "      tester.append([num,dice,numerator,denominator])\n",
        "  #print(tester)\n",
        "  return sum(dices)/len(dices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oCOmud6KuLTF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Following is a Experimental code for running on the server"
      ]
    },
    {
      "metadata": {
        "id": "FZNKYZtfkRa4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from numpy import array\n",
        "import keras\n",
        "import np_utils\n",
        "import sys\n",
        "\n",
        "tf.reset_default_graph()\n",
        "x1 = tf.placeholder(tf.float32)\n",
        "y1 = tf.placeholder(tf.float32)\n",
        "z1 = tf.placeholder(tf.float32)\n",
        "x2 = tf.placeholder(tf.float32)\n",
        "y2 = tf.placeholder(tf.float32)\n",
        "z2 = tf.placeholder(tf.float32)\n",
        "cube = tf.placeholder(tf.float32)\n",
        "lr = tf.placeholder(tf.float32)\n",
        "Y_ = tf.placeholder(tf.int32)\n",
        "training = tf.placeholder(tf.bool, name='training')\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "x1 = tf.reshape(x1, shape=[1,29,29,1])\n",
        "y1 = tf.reshape(y1, shape=[1,29,29,1])\n",
        "z1 = tf.reshape(z1, shape=[1,29,29,1])\n",
        "x2 = tf.reshape(x2, shape=[1,87,87,1])\n",
        "y2 = tf.reshape(y2, shape=[1,87,87,1])\n",
        "z2 = tf.reshape(z2, shape=[1,87,87,1])\n",
        "cube = tf.reshape(cube, shape=[1,13,13,13,1])\n",
        "\n",
        "def conv3d(x, n_kernels):\n",
        "  return tf.layers.conv3d(x,n_kernels,(5,5,5),padding=\"valid\",activation=tf.nn.elu)\n",
        "\n",
        "def conv2d(x, n_kernels):\n",
        "  return tf.layers.conv2d(x,n_kernels,(5,5),padding=\"valid\",activation=tf.nn.elu)\n",
        "\n",
        "def conv2d_shared(x, n_kernels,nam):\n",
        "  return tf.layers.conv2d(x,n_kernels,(5,5),padding=\"valid\",name=nam)\n",
        "\n",
        "def maxpool(x):\n",
        "  return tf.layers.max_pooling2d(x,(2,2),padding=\"valid\")\n",
        "\n",
        "def meanpool(x):\n",
        "  return tf.layers.average_pooling2d(x,(3,3),strides=3)\n",
        "\n",
        "def batchnorm(x, training):\n",
        "  return tf.layers.batch_normalization(x, training=training)\n",
        "    \n",
        "def relu(x):\n",
        "  return tf.nn.elu(x)\n",
        "\n",
        "def residual(x,y):\n",
        "  n_x = x.shape[-1]\n",
        "  n_y = y.shape[-1]\n",
        "  spatial_rank = 3\n",
        "\n",
        "  if n_x > n_y:  # pad the channel dim\n",
        "      pad_1 = np.int((n_x - n_y) // 2)\n",
        "      pad_2 = np.int(n_x - n_y - pad_1)\n",
        "      padding_dims = np.vstack(([[0, 0]], [[0, 0]] * spatial_rank, [[pad_1, pad_2]]))\n",
        "      y = tf.pad(tensor=y, paddings=padding_dims.tolist(), mode='CONSTANT')\n",
        "  elif n_x < n_y:  # make a projection\n",
        "      y = conv3d(y,1)\n",
        "\n",
        "  # element-wise sum of both paths\n",
        "  output_tensor = x + y\n",
        "  \n",
        "\n",
        "  return output_tensor\n",
        "\n",
        "def dilatedCon(x,n_kernels,dilation):\n",
        "  return tf.layers.conv3d(x, n_kernels,(3,3,3), [1, 1, 1], padding=\"same\", dilation_rate=[dilation, dilation, dilation])\n",
        "\n",
        "def finalconv(x):\n",
        "  return tf.layers.conv3d(x,1,(1,1,1),padding=\"same\")\n",
        "\n",
        "def softmax(x):\n",
        "  return tf.nn.softmax(x)\n",
        "\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  with tf.variable_scope(\"conv1\",reuse=tf.AUTO_REUSE):\n",
        "    x1_1 = conv2d_shared(x1,20,\"layer1\")\n",
        "    y1_1 = conv2d_shared(y1,20,\"layer1\")\n",
        "    z1_1 = conv2d_shared(z1,20,\"layer1\")\n",
        "  with tf.variable_scope(\"conv2\",reuse=tf.AUTO_REUSE):\n",
        "    x2_1 = conv2d_shared(meanpool(x2),20,\"layer2\")\n",
        "    y2_1 = conv2d_shared(meanpool(y2),20,\"layer2\")\n",
        "    z2_1 = conv2d_shared(meanpool(z2),20,\"layer2\")\n",
        "\n",
        "  cube_1 = conv3d(cube,20)\n",
        "  \"\"\"\n",
        "  x1_1 = batchnorm(x1_1,training)\n",
        "  y1_1 = batchnorm(y1_1,training)\n",
        "  z1_1 = batchnorm(z1_1,training)\n",
        "  x2_1 = batchnorm(x2_1,training)\n",
        "  y2_1 = batchnorm(y2_1,training)\n",
        "  z2_1 = batchnorm(z2_1,training)\n",
        "  cube_1 = batchnorm(cube_1,training)\n",
        "  \"\"\"\n",
        "  x1_2 = conv2d(x1_1,50)\n",
        "  y1_2 = conv2d(y1_1,50)\n",
        "  z1_2 = conv2d(z1_1,50)\n",
        "  x2_2 = conv2d(x2_1,50)\n",
        "  y2_2 = conv2d(y2_1,50)\n",
        "  z2_2 = conv2d(z2_1,50)\n",
        "  cube_2 = conv3d(cube_1,50)\n",
        "  \n",
        "  x1_2 = tf.layers.flatten(x1_2)\n",
        "  y1_2 = tf.layers.flatten(y1_2)\n",
        "  z1_2 = tf.layers.flatten(z1_2)\n",
        "  x2_2 = tf.layers.flatten(x2_2)\n",
        "  y2_2 = tf.layers.flatten(y2_2)\n",
        "  z2_2 = tf.layers.flatten(z2_2)\n",
        "  cube_2 = tf.layers.flatten(cube_2)\n",
        "\n",
        "  conv = tf.concat([x1_2, y1_2, z1_2, x2_2, y2_2, z2_2, cube_2],axis=1)\n",
        "  \"\"\"\n",
        "  conv = tf.layers.batch_normalization(conv, training=training)\n",
        "  \"\"\"\n",
        "  fc1 = tf.contrib.layers.fully_connected(conv,3000,activation_fn=None)\n",
        "  fc1 = tf.layers.batch_normalization(fc1, training=training)\n",
        "  fc1 = relu(fc1)\n",
        "  \n",
        "  fc2 = tf.contrib.layers.fully_connected(fc1, 3000,activation_fn=None)\n",
        "  fc2 = tf.layers.batch_normalization(fc2, training=training)\n",
        "  fc2 = relu(fc2)\n",
        "  \n",
        "  logits = tf.contrib.layers.fully_connected(fc2,135,activation_fn=tf.nn.elu)\n",
        "  soft_logits = tf.nn.softmax(logits)\n",
        "\n",
        "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tf.cast(Y_,tf.int32)))\n",
        "  optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "  optimizer = optimizer.minimize(loss)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()\n",
        "gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "sess.run(init)\n",
        "\n",
        "def whitening(image):\n",
        "    \"\"\"Whitening. Normalises image to zero mean and unit variance.\"\"\"\n",
        "\n",
        "    image = image.astype(np.float32)\n",
        "\n",
        "    mean = np.mean(image)\n",
        "    std = np.std(image)\n",
        "\n",
        "    if std > 0:\n",
        "        ret = (image - mean) / std\n",
        "    else:\n",
        "        ret = image * 0.\n",
        "    return ret\n",
        "\n",
        "\n",
        "dataset = np.load(\"drive/My Drive/Colab Notebooks/Dataset_final_1.npy\")\n",
        "npad = ((43,43), (43,43), (43,43))\n",
        "print(\"dataset length \",len(dataset))\n",
        "training_set = []\n",
        "\n",
        "hm_epochs = 1\n",
        "starting = 0\n",
        "no_samples = 10000\n",
        "\n",
        "if(starting>0):\n",
        "  saver.restore(sess, \"Training Models/model-\"+ str(starting)+\".ckpt\")\n",
        "  print(\"Model Restored\")\n",
        "\n",
        "print(\"Start Training\")\n",
        "try:\n",
        "  for epoch in range(hm_epochs):\n",
        "      e_time = time.time()\n",
        "      #print(\"Start Training\")\n",
        "      correct = 0\n",
        "      iter = 0\n",
        "      stime = time.time()\n",
        "      for i in range(0,len(dataset)):\n",
        "        total = np.size(dataset[i][0])\n",
        "        print(\"File \" + str(i))\n",
        "        \n",
        "        for j in range (0,1000):\n",
        "          value = 0\n",
        "          stime = time.time()\n",
        "          while(value==0):\n",
        "            voxel = random.randint(0,total)\n",
        "            output = np.zeros((1,135),dtype=np.int32)\n",
        "\n",
        "            #find the voxel x,y,z values\n",
        "            z_layer = (voxel//65536)\n",
        "            row = (voxel%65536)//256\n",
        "            element = (voxel%65536)%256\n",
        "            value = dataset[i][1][z_layer][row][element]\n",
        "\n",
        "          \n",
        "          raw = np.pad(dataset[i][0], pad_width=npad, mode='constant', constant_values=0)\n",
        "          #raw = raw/5500\n",
        "          raw = whitening(raw)\n",
        "          n_values = 135\n",
        "          output = [value]\n",
        "\n",
        "          small_x = raw[z_layer+43][row+29:row+58,element+29:element+58]\n",
        "          small_y = raw[z_layer+29:z_layer+58,row+29:row+58,element+43]\n",
        "          small_z = raw[z_layer+29:z_layer+58,row+43,element+29:element+58]\n",
        "          large_x = raw[z_layer][row:row+87,element:element+87]\n",
        "          large_y = raw[z_layer:z_layer+87,row:row+87,element]\n",
        "          large_z = raw[z_layer:z_layer+87,row,element:element+87]\n",
        "          vol = raw[z_layer+37:z_layer+50,row+37:row+50,element+37:element+50]\n",
        "\n",
        "\n",
        "          small_x = np.reshape(small_x, (1,29,29,1))\n",
        "          small_y = np.reshape(small_y, (1,29,29,1))\n",
        "          small_z = np.reshape(small_z, (1,29,29,1))\n",
        "          large_x = np.reshape(large_x, (1,87,87,1))\n",
        "          large_y = np.reshape(large_y, (1,87,87,1))\n",
        "          large_z = np.reshape(large_z, (1,87,87,1))\n",
        "          vol = np.reshape(vol, (1,13,13,13,1))\n",
        "          \n",
        "          \n",
        "          pred = (soft_logits.eval({x1: small_x, y1: small_y, z1: small_z, x2: large_x, y2: large_y, z2: large_z, cube: vol, keep_prob: 1.0, training:False },session=sess))\n",
        "          #print(pred)\n",
        "          print(\"Precited: \", np.argmax(pred))\n",
        "          _,loss_value = sess.run([optimizer, loss], feed_dict={x1: small_x, y1: small_y, z1: small_z, x2: large_x, y2: large_y, z2: large_z, cube: vol , Y_ : output, lr:0.01 , keep_prob:0.9, training:True})\n",
        "          print(\"Class: \", value)\n",
        "          print(time.time()-stime)\n",
        "          print(str(loss_value))\n",
        "          if(value==np.argmax(pred)):\n",
        "            correct += 1\n",
        "          iter += 1\n",
        "          print(\"Correct Prediction % : \", (correct/iter)*100)\n",
        "          if((j+1)%100 == 0):\n",
        "            print(str(j+1) + \" complete out of 20000. Loss is: \" + str(loss_value))\n",
        "          if((j+1)%5000 == 0):\n",
        "            print(\"Training 5000 iterations complete Saving Model with \" + str(j+1) +\" Iterations\")\n",
        "            save_path = saver.save(sess, \"Training Models/model-\"+ str(j+1)+\".ckpt\")\n",
        "            print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "            \n",
        "          \n",
        "            \n",
        "      print(\"One Epoch Completed: Time Taken %s s\" % (time.time()-e_time))\n",
        "      e_time = time.time()\n",
        "      print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',str(loss_value))\n",
        "\n",
        "\n",
        "  print(\"Training Complete Saving Model with \" + str((iter+1)*hm_epochs*len(dataset)) +\" Iterations\")\n",
        "  save_path = saver.save(sess, \"Training Models/model-\"+ str((iter+1)*hm_epochs*len(dataset))+\".ckpt\")\n",
        "  print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "\n",
        "  print(\"Done!\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"Training Interrupted Saving Model with \" + str(j+1+starting) +\" Iterations\")\n",
        "  #save_path = saver.save(sess, \"Training Models/model-\"+ str(j+1+starting)+\".ckpt\")\n",
        "  #print(\"Model saved in path: %s\" % save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hEVUVfpMlYzP",
        "colab_type": "code",
        "outputId": "59e70719-c2c1-4c7f-88b0-c8d9a797ac6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "with tf.Session() as sess:\n",
        "  devices = sess.list_devices()\n",
        "  \n",
        "print(devices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 7531884453388044863), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8779326169682415652), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 634049089981660065), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 11276946637, 16859271132039702870)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}